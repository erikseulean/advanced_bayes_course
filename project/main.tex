\documentclass[12pt,a4paper]{article}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\graphicspath{ {./images/} }
\renewcommand{\baselinestretch}{1.5}
\everymath{\displaystyle}

\author{Erik-Cristian Seulean}
\title{Nonparametric Bayesian inference - a guide to clustering algorithms}
\date{\today}

\begin{document}
\maketitle
\section{Motivation}

In the last years, in every corner of the internet there are enormous quantities of data created. 
Regardless of the nature of this data, as quantities increase over time, we're challenged to find
ways to group data into well-defined categories. The categorization of data allows us to explore 
datasets in a more organized way and reduces the time required to find particular datapoints. 
Today, there are over 6 million articles on Wikipedia. Regardless of the size of this article
collection, everything is grouped in ways that allows us to explore different interests in
organized ways. The library, here in St. Andrews contains over 1 million books, yet finding a book
today on a topic such as Bayesian inference is a matter of minutes. As the University increases the
diversity of degrees that students can take, the number of book categories increase over time and 
this leads to the following question \textit{How do we generate a mathematical model that is capable
to group books that are alike into well-defined categories ?}

\section{Nonparametric Bayes a brief description}
There are two separate classes of Bayesian inference. One class contains inference on
parameters of distributions, where the prior and posterior distributions are functions of parameters of
interest with fixed dimension. In this case we can consider the following: 

\begin{itemize}
    \item $\theta = (\theta_{1}, \theta_{2}, ..., \theta_{n})$ - the parameter that we are interested in 
    \item $P(\theta)$ - prior probability
    \item $\pi(\theta|X)$ - posterior probability
\end{itemize}

In the above example, the parameter has a fixed determined dimension. We know what probability distributions
work for the number of parameters we have. For a single unknown parameter for example we can use a Uniform, 
Poission or Exponential distribution. For two parameters we could use a Normal or Beta distribution (among others)
depending on what sort of data we are modelling. 
In this situation we can use Bayes formula to define the posterior distribution based on the prior and likelihood:

$$\pi(\theta|x_{1}, x_{2}, ..., x_{n}) \propto \prod_{i}^{n}f(x_{i}|\theta)p(\theta)$$ 

But what happens when you don't know beforehand how many parameters you need ? How many parameters will describe
your data well ? Are your observations dependent on one parameter or 3 parameters ? Nonparametric Bayesian analysis
is focused on problems where the number of parameters are not known beforehand. In other words, we can assume that
there can be any number of parameters, we will examine the data and draw some conclusions on how many parameters of 
interest describe the data well. 
In more rigorous mathematical terms, so far we can define the following:

\begin{itemize}
    \item $X_{1}, X_{2}, X_{3}, ... X_{n}$ are i.i.d observations from an \textit{unknown} distribution F
    \item F is not from a finite space, we cannot index it with a finite number of parameters
\end{itemize}

Similar to the parametric case, we would like to apply Bayesian methods. This means that we need to 
specify a prior distribution, but because we don't know the number of parameters that we are dealing with,
we cannot specify priors on parameters, we need to specify a prior on the distribution itself. What in 
the parametric case was $P(\theta)$ becomes now $P(F)$ where F is a distribution! 
To put it in the context of the example from the beginning of the essay, if we start making an analysis on
the topics of the books in the library, the more books we examine the more topics we can encounter. That doesn't
mean that every book is a new topic, but the number of topics grows as we examine more books. This is equivalent
to the number of parameters in a nonparametric Bayesian model where the number of parameters is unknown but growing
with the data. 

\section{The clustering problem}
Assuming we have observations $X_{1}, X_{2}, ..., X_{n}$ we want to assign every datapoint to a single cluster. In a general Gaussian mixture model, with the number of clusters $K$ set to 2, we could potentially have the following setup: 

$$\mu_{k} \sim \mathcal{N}(\mu_{0}, \sigma_{0}^2)$$ 
$$\rho_{1} \sim Beta(a, b)$$
$$\rho_{2} = 1 - \rho{1}$$ 
$$Z_{n} \sim Categorical(\rho_{1}, \rho_{2})$$
Here we're choosing $\rho_{1}$ coming from a Beta distribution as it is mathematically convenient, but in practice, it would require further analysis to figure out if this is a valid assumption. Depending on the parameters of this Beta distribution, it would put more or less mass on $\rho_{1}$ compared to $\rho_{2}$.

In case we want to generalize the model above to have K clusters, we can replace the Beta distribution above with a Dirichlet distribution. In this case, instead of drawing only one observation for $\rho_{1}$, we could draw K observation for $\rho_{1}, \rho_{2}, \rho_{3}, ..., \rho_{K}$.

$$\mu_{k} \sim \mathcal{N}(\mu_{0}, \sigma_{0}^2)$$ 
$$\rho_{1:K} \sim Dirichlet(a_{1}, a_{2}, ..., a_{K})$$
$$Z_{n} \sim Categorical(\rho_{1}, \rho_{2}, ... ,\rho_{K})$$

The Dirichlet distribution used above is the natural generalization of Beta distribution to a multiparameter case:

$$ Dir(\rho_{1}, \rho_{2}, ..., \rho_{n}| a_{1}, a_{2}, ..., a_{n}) = \frac{\Gamma(\sum_{i=1}^{K}a_{i})}{\prod_{i=1}^{K}\Gamma(a_{i})}\prod_{i}^{K}\rho_{i}^{a_{i} - 1}$$ where
$\sum_{i}^{K} \rho_{i} = 1, a_{k} > 0$ and $\rho_{k} \in (0, 1)$.

The \textit{rdirichlet} function provided with the library can generate samples from a Dirichlet distribution.

So far the examples above, all work on the case where the number of clusters is fixed and known. We are capable of drawing observations from the Dirichlet distribution, because we know how many parameters we are working with. In the figure below, the first delimitation corresponds to the first cluster, second to the second cluster and so on, up to 100 clusters.

\begin{figure} [H]
    \begin{center}
        \includegraphics[scale=1, width=13cm]{stacked_dirichlet.png}
        \caption{Clusters generated from a Dirichlet distribution.}
        \label{fig:stacked_dirichlet}
    \end{center}
\end{figure}

At this point, we only considered the available clusters, but we have not sampled our datapoints $X_{1}, X_{2}, ..., X_{n}$ into the corresponding clusters. To put a particular datapoint into a certain cluster, we can draw an observation from a Uniform distribution and check which cluster it belongs to. We will sum up every cluster delimitation until the total sum is higher than the value sampled from the \textit{Uniform distribution}. When we exceeded the sum, the latest $\rho_{k}$ will denote the $k^{th}$ cluster where the datapoint belongs to. We can repeat the same until all n datapoints are assigned to a cluster. The cluster generation function in the library allows to visually get an intuition of how this happens sequentially. 

\begin{figure} [H]
    \begin{center}
        \includegraphics[scale=1, width=13cm]{clustered_points.png}
        \caption{4 points associated with the clusters generated}
        \label{fig:clustered_points}
    \end{center}
\end{figure}

While this model looks appealing, there is always going to be an upper bound on K, that defines the number of clusters. In this scenario, there is no reliable way to choose K that would be sufficient in any general case. Even if we can come up with a value of K large enough, there are computational concerns when it comes to storing a large amount of information in memory that is associated with K.

\section{Setting K=$\infty$}
A solution to the above limitation is to set K to be arbitrary large that would not limit us from having a very large number of clusters. In this case we need a different procedure to generate $\rho_{1}, \rho_{2}, ..., \rho_{K}$ from a Dirichlet distribution. What we want to do is take the interval $(0, 1)$ and break it into an infinite number of subintervals that sum up to 1. In other words, we first break a proportion of the interval $(0, 1)$ and denote that being $\rho_{1}$. Out of the remaining part $(\rho_{1}, 1)$ we break another proportion and denote that to be $\rho_{2}$. We can repeat this as many times as we want, and we can generate an infinite number of proportions, that sum up to 1, the length of the initial interval. This algorithm is called the \textit{"stick-breaking process"} and can be summarized as follows: 

\begin{itemize}
    \item Generate $P_{1} \sim Beta(a_{1}, b_{1})$. Set $\rho_{1}$ to this value.  
    \item Generate $P_{2} \sim Beta(a_{2}, b_{2})$. Set $\rho_{2} = (1 - P_{1})P_{2}$
    \item Generate $P_{3} \sim Beta(a_{3}, b_{3})$. Set $\rho_{3} = (1 - P_{1})(1-P_{2})P_{3}$
    \item Set $\rho_{K} = 1 - (\rho_{1} + \rho_{2} + ... + \rho_{K-1})$
\end{itemize}

The choice of parameters $a$ and $b$ for the Beta distribution are researched by Ishwaran and James, 2001 \cite{ishwaran2001}, and we will simply use $1$ and $\alpha$ with $\alpha$ given for all $a_{k}$ and $b_{k}$. 
When $a_{k}=1$ and $b_{k}=\alpha$, $\alpha > 0$ the process is called \textit{Dirichlet process stick-breaking}. The distribution over all $\rho = (\rho_{1}, \rho_{2}, ...)$ is called Griffin-Engen-McCloskey (GEM). 

$$ (\rho_{1}, \rho_{2}, ...) \sim GEM(\alpha)$$

\section{Dirichlet process mixture model}

The Gaussian mixture model specified above, for K fixed clusters can be transformed to work with an arbitrary number of clusters by using the $GEM$ distribution instead of Dirichlet.

$$(\rho_{1}, \rho_{2}, ...) \sim GEM(\alpha)$$
$$\mu_{k} \sim \mathcal{N}(\mu_{0}, \sigma^2)$$
$$ G = \sum_{k}^{\infty}\rho_{k}\delta_{\mu k} \sim DPM(\alpha, \mathcal{N}(\mu_{0}, \sigma^2))$$

The above will put weight $\rho_{k}$ at $\delta_{\mu k}$ where $\delta_{\mu k}$ denotes the $k^{th}$ cluster. The first parameter is $\alpha$, parameter needed for the GEM distribution, the second one is the distribution used to draw $\delta_{\mu k}$, the locations of the clusters. In practice, sampling from this distribution would mean that with probability $\rho_{k}$ will draw an observation from cluster with center $\delta_{\mu k}$.

The problem that arises with this model is that we cannot draw an infinite number of probabilities $\rho_{k}$, but we can do draw them on demand, as we sample. The example above, where we draw points into the cluster, by sampling from a Uniform distribution and check which clusters the points belong to can give an indication of what we can do here. If we first draw a point $u \sim Uniform(0, 1)$ we can then sample enough $\rho_{k}$ values that would make the sum of all $\rho_{k}$ sampled be greater than $u$. The complete step by step sampling algorithm is given by the following:

\begin{enumerate}
    \item Draw an observation $u$ from a $Uniform(0, 1)$ distribution.  
    \item Generate $P_{k} \sim Beta(1, \alpha)$ and find $\rho_{k}$ using the \textit{stick breaking algorithm}. Repeat until $\sum_{k}\rho_{k} > u$. 
    \item For every $\rho_{k}$ sampled, sample the center for the cluster associated with $\rho_{k}$ from $\mathcal{N}(\mu_{0}, \sigma_{0}^2)$ 
    \item Find the cluster the point belongs to such that $\rho_{k-1} < u < \rho_{k}$.
    \item Sample an observation from $\mathcal{N}(\mu_{k}, \sigma^2)$ where $\mu_{k}$ is the center of the cluster that was selected.
    \item Repeat until all $n$ points are sampled.
\end{enumerate}

The algorithm above can be observed step by step using \textit{rDPMM\_visual} in the package provided, while \textit{rDPMM} returns a sample.

The \textit{Dirichlet process} as presented above, is a distribution of distributions over a sample space. If we consider $A_{1}, A_{2}, ..., A_{n}$ a partition of the sample space $\Omega$, we can say that the $Dirichlet\ process$ assigns a particular mass, to each of this partitions of the sample space. The total mass distributed to each partition is distributed following a \textit{Dirichlet\ distribution}. 

$$(P(A_{1}), P(A_{2}), ... , P(A_{k})) \sim Dir(\alpha G(A_{1}), \alpha G(A_{2}), ..., \alpha G(A_{k}))$$
Given that there is a connection between the \textit{Dirichlet process} and the \textit{Dirichlet distribution}, we can derive the posterior distribution when we have Dirichlet-Categorical or Dirichlet-Multinomial prior and likelihood pairs:

$$\rho_{1}, ... ,\rho_{k} \sim Dir(\alpha_{1}, ..., \alpha_{k})$$
$$x_{1}, ... ,x_{k} \sim Categorical(\rho_{1}, ... \rho_{k})$$

$$\displaystyle \pi(\rho|x) \propto f(x|\rho)\pi(\rho) \propto \prod_{i}^{k} f(x_{i}| \rho)\pi(\rho) $$
$$\propto \prod_{i}^k \prod_{j}^k \rho_{j}^{I(x_{i} = j)}\prod_{i}^k \rho_{i}^{\alpha_{i} - 1} \propto \prod_{j}^k \rho_{j}^{\sum_{i}I(x_{i} = j)}\prod_{i}^k \rho_{j}^{\alpha_{j} - 1}$$
$$ \propto \prod_{j}^k \rho_j^{{c_{j}} + \alpha{j} - 1}$$
$c_{j} = \sum_i I(x_{i} == j) $

Similarly, if the observations are from a \textit{Multinomial distribution}:
$$x_{1}, ... ,x_{k} \sim Multinomial(\rho_{1}, ... \rho_{k})$$
$$\displaystyle \pi(\rho|x) \propto f(x|\rho)\pi(\rho) \propto \prod_{i}^{k} f(x_{i}| \rho)\pi(\rho) $$
$$ \propto \prod_{i}^{k} \rho_{i}^{c_{i}} \prod_{i}^{k} \rho_{i}^{\alpha - 1} \propto \prod_{i}^{k} \rho_{i}^{c_{i} + \alpha_{i} - 1}$$ 

In both cases, we can notice that the posterior is following a \textit{Dirichlet distribution} so the Dirichlet distribution is the Conjugate prior for both the Categorical and the Multinomial distributions. The posterior distribution in these cases is $\pi(\rho|x) \sim Dir(\alpha_{1} + c_{1}, ..., \alpha_{k} + c_{k})$. In terms of the clusters (or partition of the sample space), if we observe a new observation in cluster $i$, the posterior then is updated with a new observation in cluster $i$:
$$(P(A_{1}), P(A_{2}), ... , P(A_{k})) \sim Dir(\alpha G(A_{1}), \alpha G(A_{2}), ..., G(A_{i}) + 1, ..., \alpha G(A_{k}))$$ which in fact leads to $ G|x_{1} \sim DP(\alpha + 1, \frac{\alpha \mathcal{N} + \delta_{x}}{\alpha + 1})$.

\section{Pólya Urn}

In the section above, drawing observations from the \textit{Dirichlet process} required to sample values for $\rho_{k}$ from the \textit{GEM distribution}. If we could find a way to avoid this step, we could remove the $\rho_{k}$ variables altogether and the model would be simpler. It turns out that there is a way of doing this by integrating out $\rho_{k}$. As before, we could start from the same setup as in the Gaussian mixture model:

$$\rho_{1} \sim Beta(a, b)$$
$$Z_{n} \sim Categorical(\rho_{1}, \rho_{2})$$

\begin{align*}
\begin{array}{l@{{}={}}l}
    P(Z_{n} = 1|Z_{1}, Z_{2}, ..., Z_{n-1})
    & \int P(Z_{n} = 1, \rho_{1}|Z_{1}, Z_{2}, ..., Z_{n-1}) \,d\rho_{1} \\
    & \int P(Z_{n} = 1|\rho_{1})P(\rho_{1}|Z_{1}, Z_{2}, ..., Z_{n-1}) d\rho_{1} \\
    & \int \rho_{1} Beta(a + c_{1}, b + c_{2})d\rho_{1} \\
    & \int \rho_{1} \frac{\Gamma(a + c_{1} + b + c_{2})}{\Gamma(a+c_{1})\Gamma(b+c_{2})}\rho_{1}^{a+c_{1}}(1-\rho_{1})^{b+c_{2}} d\rho_{1} \\
    & \frac{\Gamma(a + c_{1} + b + c_{2})}{\Gamma(a+c_{1})\Gamma(b+c_{2})} \frac{\Gamma(a+c_{1}+1)\Gamma(b+c_{2})}{\Gamma(a + c_{1} + b + c_{2}+1)} \\
    & \frac{a + c_{1}}{a + c_{1} + b + c_{2}}
\end{array}
\end{align*}

where $c_{j} = \sum_{i}^{n-1}I(Z_{i} = j)$

In this case we are independent on $\rho_{k}$, and we can put an observation $Z_{k}$ into the cluster one with probability derived above. Intuitively, this can be interpreted in the following way. If we start with an urn, containing initially \textit{a} number of red balls and \textit{b} blue balls and draw from a \textit{Bernoulli distribution} with probability derived above, we can keep adding red or blue balls to the urn, based on the value sampled. As the number of balls in the urn goes to infinity, the proportion of red balls in the urn is $\rho_{1}$. An alternative intuition is that using the initial setup, we can draw a ball from the urn inspect the color and return to the urn two balls of that same color. As the number of balls grows to infinity the proportion of red balls goes to $\rho_{1}$. Not just the proportion of red to blue balls is $\rho_{1}$ as the number of balls goes to infinity, the distribution of the proportion of red balls is also \textit{Beta(a, b)}. The distribution of red balls in the urn is noted as being \textit{PólyaUrn(a, b)}.

The Pólya urn for two types of balls can be generalized to K different colors, but instead of using Beta we can use $\rho_{1}, ..., \rho_{k} \sim Dirichlet(\alpha_{1}, ..., \alpha_{k})$. The probability of $n^{th}$ observation to go into cluster k given the previous \textit{n-1} observations is the same:
$P(Z_{n} = k|Z_{1}, ..., Z_{n-1}) = \frac{\alpha_{k} + c_{k}}{\sum_i^n (\alpha_{i} + c_{i})}$. If we keep adding balls to the urn, in the same procedure as above, the proportion of balls in the urn will follow the \textit{Dirichlet distribution} that we started with. 

As before, we built the model up starting with a \textit{Beta distribution} for two clusters, then we generalized to K clusters using a \textit{Dirichlet distribution}. The next logical step is to set $K=\infty$. To be able to handle an arbitrary number of clusters, we will use a different type of urn called \textit{Hoppe urn} or \textit{Blackwell-MacQueen urn}. The urn will initially contain only one magic ball. When the magic ball is extracted from the urn, we put it back together with a ball of a new color, that does not exist in the urn. If we extract a ball of a particular color, we will return it to the urn together with another ball of the same color. The distribution of balls of any color is \textit{PólyaUrn(1, $\alpha$)}. The proportion of balls of the first color is also $Beta(1, \alpha)$, and this takes us back to the \textit{Stick breaking process.} If we consider $\rho_{k}$ the proportion of balls of the $k^{th}$ color we have the following: 

$$ V_{k} \sim Beta(1, \alpha) $$
$$\rho_{1} = V_{1}$$
$$\rho_{2} = (1 - V_{1})V_{2}$$
$$\rho_{k} = \prod_{i=1}^{k-1}(1-V_{i})V_{k}$$

The urn is identical to the Dirichlet process, but we don't have to deal with the \textit{GEM distribution} anymore as the $\rho_{k}$ were integrated out.

\section{The chinese restaurant process}
The \textit{chinese restaurant process} works in the same way as the \textit{Pólya urn}. A customer walks into a restaurant and either sits down at a new table or sits at a table that is already occupied, proportional to the number of people that already sit at the table. This process is identical to the Blackwell-MacQueen urn, we can either draw the magic ball (sit at a new table) or draw a different colored ball (sit at an already occupied table). Each table in this case will have a color, and as in the previous example we have infinitely many tables. A librarian working at the library in St. Andrews, can either assign a new book to an existing category or create a new category for that book. 

It comes naturally to ask what is the difference between the \textit{Dirichlet process} and the \textit{Chinese Restaurant process}. While the \textit{Dirichlet process} describes both the cluster assignments and the proportion of the clusters the \textit{Chinese Restaurant process} describes only the cluster assignments. In many applications, knowing the cluster proportions is not of much use, so applications generally rely on implementations of the CRP algorithm for practical purposes. I was curious to find out where the name for the algorithm comes from, and I found out that the Chinese restaurants in San Francisco, in the nineties were trying to accommodate as many customers as possible. Customers were generally interested in the popular tables (with many other customers) but now and then, a customer would come in and sit at a new table. 

Given that we are only considering the cluster arrangements and not the proportions for each cluster, we are interested in modelling the partitions of the datapoints. For example, one partition of 10 datapoints can be the following $\{\{1, 5, 7\}, \{2,3,4,6\}, \{8, 9\}\}$. That denotes that the first datapoint, the $5^{th}$ and the $7^{th}$ are assigned in cluster one, and so forth. The order of the clusters doesn't matter, as the clusters are interchangeable, the datapoints will be associated with the same cluster regardless. It comes naturally to ask what is the probability of a cluster arrangement. The probability for the cluster arangement above is the following: $$\frac{\alpha}{\alpha}\frac{\alpha}{\alpha+1}\frac{1}{\alpha+2}\frac{2}{\alpha+3}\frac{1}{\alpha+5}\frac{3}{\alpha+6}\frac{2}{\alpha+7}\frac{\alpha}{\alpha+8}\frac{1}{\alpha+9}$$

From here we can easily generalize to N datapoints. Considering K clusters and $n_{k}$ datapoints in cluster k we have the following: $$\frac{\alpha^{K} \prod_{k}(n_{k}-1)!}{\alpha(\alpha+1)...(\alpha+N-1)} = P(\prod\nolimits_{N} = \pi_{N})$$

We notice here that the order in which datapoints are assigned to clusters doesn't change the probability of a particular arrangement. If we observe the partition of the 10 datapoints as above, the probability for the partition is the same if we permute the datapoints and consider them in a different order, as long as the datapoints are assigned to the same clusters as initially. This gives a property called exchangeability that comes handly in Gibbs sampling, which will be discussed in the later pages. This allows us to consider any datapoint, as being always the last one: 
$$P(\prod\nolimits_{N}|\prod\nolimits_{N, -n})= \left \{
    \begin{array}{ll}
		\frac{\#C}{\alpha+ N - 2} & \mbox{if joins an existing cluster} \\
		\frac{\alpha}{\alpha+N-2} & \mbox{if starts a new cluster} 
	\end{array}
\right. $$ Where $\#C$ denotes the number of datapoints in that cluster until this point.

\section{Gibbs sampling for the Chinese restaurant process}
Gibbs sampling for the Chinese restaurant process allows us to perform inference about our data. For the given datapoints, we can run the sampler and try to recover the clusters in the data. We will consider that the cluster means are drawn from a \textit{Normal distribution} with known mean and variance $\mu_{0}$ and $\Sigma_{0}$. In practice, this model might be simplistic, and we would probably set a prior on $\mu_{0}$ and $\Sigma_{0}$ as well that would perform better on real datasets. Again, for simplicity purposes, we consider that the datapoints $X_{1}, X_{2}, ..., X_{n}$ are drawn from a \textit{Normal distribution} with known $\Sigma$ and unknown mean. This can be sumarised as follows: 

$$ \mu_{c} \sim \mathcal{N}(\mu_{0}, \Sigma_{0})$$
$$ X_{k}|\mu_{c} \sim \mathcal{N}(\mu_{c}, \Sigma) $$

This setup allows us to take advantage of the fact that the Normal distribution is the conjugate prior for the Normal distribution. In other words, the posterior distribution will be a \textit{Normal distribution} as well, with parameters updated based on the likelihood and prior.

The posterior distribution in this case is $P(\prod\nolimits_{N}|X_{1}, X_{2}, ..., X_{n})$. This posterior gives us the assignment of datapoints to clusters, given the existing datapoints. To be able to compute this posterior we will consider that we have the cluster assignments for $n-1$ datapoints, and we are only interested in assigning the current datapoint to a cluster. 

$$P(\prod\nolimits_{N}|\prod\nolimits_{N, -n}, X_{1}, X_{2}, ..., X_{n})$$

The $n^{th}$ datapoint has two possible options, either join an existing cluster or start a new cluster itself. In that case we can write the probability as follows: 

$$P(\prod\nolimits_{N}|\prod\nolimits_{N, -n}, X_{1}, ..., X_{n}) = \left \{
    \begin{array}{ll}
		\frac{\#C}{\alpha+ N - 2}P(C \cup \{X_{n}\}|C) & \mbox{if joins a cluster} \\
		\frac{\alpha}{\alpha+N-2}P(\{X_{n}\}) & \mbox{if starts a new cluster} 
	\end{array}
\right.$$
where $\#C = \sum_{i}^{n}\mathcal{I}(X_{i} \in C)$

$P(C \cup \{X_{n}\}|C)$ is the posterior distribution derived from the likelihood and the prior by completing the circle:
$$P(C \cup \{X_{n}\}|C) \sim N(\mu, \Sigma_{x} + \Sigma_{0})$$
$$\Sigma_{x}^{-1} = \Sigma_{0}^{-1} + (\#C)\Sigma^{-1}$$
$$\mu = \Sigma(\Sigma_{x}^{-1}\sum_{k \in C}X_{k} + \Sigma_{0}^{-1}\mu_{0})$$
where $\Sigma^{-1}$ is the precision.

We can see that as the number of datapoints in the cluster grows, $\Sigma_{0}^{-1} + (\#C)\Sigma^{-1}$ will grow so $\Sigma_{x}$ will go to 0. Similarly, $\mu$ will go towards the empirical mean of the cluster.

The implementation of the Gibbs algorithm can be found in the \textit{cluster\_datapoints} function provided with the library and is as follows:

\begin{enumerate}
    \item Initialise all the datapoints in one single cluster.
    \item Iterate from 1 to \textit{maxIterations} where \textit{maxIterations} denotes how many iterations the Gibbs sampling should perform. 
    \item Iterate through all the datapoints one by one. For every datapoint remove it from the cluster is currently assigned.
    \item Calculate the log probabilities for the point belonging to the currently existing clusters and the log probability of starting a new cluster.
    \item Normalise and transform the probabilities.
    \item Sample the cluster from a Categorical distribution with parameters set to the probabilities derived above.
    \item Assign the point to that cluster and continue to the next point.
    \item When all the existing points are checked a new iteration of the Gibbs sampler is executed. 
\end{enumerate}

As mentioned earlier the implementation is simplistic, considering only one prior for the mean, with variance considered known. Alternatives to this algorithm include hierarchical models where we can set priors on both variance and mean and increase the performance of the clustering algorithm. An alternative to initialise every point belonging to a cluster is to randomly assign each datapoint to a random cluster and start the sampler with this configuration. We would expect that the sampler would adjust the number of cluster over multiple iterations and converge to the number of clusters presented by the data.

\section{Results}
In order to assess the performance of the clustering algorithm I tried the library on different datasets and I considered two options to assess how well it performs. First alternative was to consider the average number of iterations a datapoint spent in a particular cluster. This ended up being problematic because over a high number of iterations, the datapoints might switch clusters. Intuitively, consider that after $n$ iterations, all points are correctly assigned to two clusters $A$ and $B$. If I run the sampler for a high number of iterations, the points associated with cluster $A$ might switch to cluster $B$ and vice-versa. An alternative solution was to stop the clustering algorithm after a number of iterations and inspect the latest cluster association. This solution (at least on the datasets that I considered) seems to be giving relatively good results (at least visually) but in practice, the first solution might be more accurate if the cluster switching can be avoided.

First dataset that I tried the clustering algorithm on was a dataset that I generated. I tried to keep the cluster centers non-overlapping and relatively separated to have an initial assessment if it works to a certain degree. This dataset is available in the library as $split\_data$.

\begin{figure} [H]
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{initial_split_data.png}
      \caption{Before clustering}
      \label{fig:before clustering}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{split_data_clustered}
      \caption{After 1000 iterations}
      \label{fig:split points after clustering}
    \end{minipage}
\end{figure}
    
Surprisingly (or not) this worked perfectly. The cluster separation is correct and as expected. At this point, I started getting excited, so I decided to test the clustering algorithm on some datasets that I did not generate. My first thought was to try the clustering algorithm on the iris dataset. This dataset was introduced by Ronald Fisher and contains 150 samples from 3 Iris species and is available in R - $data(iris)$. Admittedly, while there are 3 different Iris species in the dataset, I was expecting to see two clusters as 2 of the different Iris species are very similar in variables and I wouldn't expect to be able to distinguish between the 3 species without a hierarchical model.

\begin{figure} [H]
    \centering
    \textbf{Iris Petal Width vs Length}\par\medskip
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{petal_before.png}
      \caption{Before clustering}
      \label{fig:petal before clustering}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{petal_after.png}
      \caption{After 1000 iterations}
      \label{fig:petal after clustering}
    \end{minipage}
\end{figure}

\begin{figure} [H]
    \centering
    \textbf{Iris Sepal Width vs Petal Width}\par\medskip
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{width_before.png}
      \caption{Before clustering}
      \label{fig:width before clustering}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{width_after.png}
      \caption{After 1000 iterations}
      \label{fig:width after clustering}
    \end{minipage}
\end{figure}

The cluster correctly finds the number of clusters, however the separation of points into clusters is not perfect. I tried running the chain for a longer time but the points on the edge of the clusters sometimes get associated with the wrong cluster. Nevertheless, the algorithm got over $90\%$ accuracy, calculated by considering how many of the 150 points were correctly associated with the correct cluster.
It's worth mentioning that these results are experimental and convergence was not checked using metrics that could tell if the stability distribution was reached.

\section{Completing the circle - clustering Books}
I started this paper by setting up a scene to cluster books, or large text documents, but the reality is that I haven't explained how the clustering algorithm, in the form of the CRP can be applied to cluster large text files. I'd like to propose a variation of the CRP algorithm that could potentially be applied to books. As In the Chinese restaurant process, let's consider the \textit{Spanish restaurant process} (given that in Spanish cuisine you usually order multiple tapas). \textit{At a Spanish restaurant a customer can order multiple dishes from a menu with unlimited possible tapas. When a customer walks into the restaurant, they can either sit down at a new table, or can sit at a table proportional to the number of tapas he likes at that table. Additionally, every occupied table has churos, on the house, which every customer likes.}

$$P(\prod\nolimits_{N}|\prod\nolimits_{N, -n}, X_{1}, ..., X_{n}) = \left \{
    \begin{array}{ll}
		\frac{\sum w_{i}\mathcal{I}({i}\in B) + 1}{\alpha + \sum_j \sum_i w_{i,j}\mathcal{I}(i \in B) + C}P(C \cup \{X_{n}\}|C)\\
		\frac{\alpha}{\alpha+\sum_i \sum_j w_{i,j}\mathcal{I}(i \in B) + C }P(\{X_{n}\}) 
	\end{array}
\right.$$
where $w_{i}$ is the count of the number of tapas $i$ existing at the table and $\mathcal{I}({i}\in B)$ the customer likes that tapas. The one in the numerator stands for the \textit{churos}, and it's used to make sure that there is a non-zero probability to sit at that table even if the customer doesn't like any tapas at that table. 

In terms of books, every book will have a list of words (after tokenization and removal of stopwords) and each word will have a count for the number of occurrences. A book can be represented by $B=(w_{0}, w_{1}, ..., w_{n})$ where $w_{i}$ is the occurrences of word \textit{i} in the book, and it will belong to a cluster, proportional to the sum of the counts of the words existing in the book that are already part of the cluster. Similar to CRP, for SRP the book will always be the last one, and we can be implemented using Gibbs sampling.

\begin{thebibliography}{9}
\bibitem{ishwaran2001}
Herman Ishwaran, Lancelot F. James \emph{Gibbs sampling methods for stick-breaking process}, 2001

\bibitem{broderick}
Tamara Broderick \emph{Nonparametric Bayesian Statistics} \url{tamarabroderick.com/files/broderick_mlss_cadiz_part_i.pdf}

\bibitem{gordon}
Gordon J. Ross, Dean Markwick \emph{dirichletprocess: An R Package for Fitting Complex Bayesian Nonparametric Models}

\bibitem{junyang}
Junyang Chen, Zhiguo Gong, Weiwen Liu \emph{A Dirichlet process biterm-based mixture model for short text stream clustering}

\bibitem{radford}
Radford M. Neal \emph{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}

\bibitem{sivakumar}
Sivakumar Murugiah \emph{Bayesian Nonparametric Clustering based on Dirichlet Processes}

\bibitem{muller}
Peter Müller, Ritten Mitra \emph{Bayesian Nonparametric Inference - Why and How}

\bibitem{jordan}
Michael Jordan, Tamara Broderick \emph{Nonparametric Bayesian Methods: Models, Algorithms, and Applications I} \url{https://simons.berkeley.edu/talks/tamara-broderick-michael-jordan-01-25-2017-1}

\end{thebibliography}

\end{document}\models